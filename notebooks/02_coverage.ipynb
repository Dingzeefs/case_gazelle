{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded demografie from parquet: (4054, 6)\n",
      "Columns: ['pc4', 'inwoners', 'income_per_capita', 'population_density', 'income_norm', 'density_norm']\n",
      "Income data available: 4030 of 4054 PC4 areas\n",
      "Loaded white spots: (23, 4)\n",
      "Merged demographic data for 23 white spots\n",
      "Mapped 23 white spots to gemeenten\n",
      "Creating custom PC4->plaats mapping...\n",
      "Applied 1 custom PC4->plaats mappings\n",
      "Loading woonplaats data for remaining areas...\n",
      "Added plaats names for 19 total areas\n",
      "✓ Apeldoorn is in ZE-steden lijst (start: 1/1/2025) - policy score = 1.0\n",
      "Merged ZE policy data: 1 ZE cities found\n",
      "\n",
      "Saved enriched white spots to: /Users/DINGZEEFS/Case_Gazelle/outputs/tables/white_spots_with_policy.csv\n",
      "\n",
      "Top 5 enriched white spots:\n",
      " pc4               gemeente     plaats  inwoners_pc4  dist_nearest_pon_km  income_norm  policy_index    score\n",
      "4561                  Hulst     Clinge          9930             9.957123     0.105911           0.0 0.450489\n",
      "6301 Valkenburg aan de Geul        NaN         10530             7.643865     0.128079           0.0 0.435665\n",
      "3253     Goeree-Overflakkee Achthuizen          6295            13.127740     0.251847           0.0 0.417122\n",
      "7351              Apeldoorn Hoenderloo          1335             8.916180     0.270320           1.0 0.414419\n",
      "6291                  Vaals    Lemiers          7900            10.026319     0.065271           0.0 0.387612\n",
      "\n",
      "Data quality summary:\n",
      "- Income data (WOZ): 23 of 23 areas\n",
      "- Policy data (ZE):  1 of 23 areas\n",
      "- Gemeente mapping:  23 of 23 areas\n",
      "- Plaats names: 19 of 23 areas\n",
      "\n",
      "🎯 PC4 7351 result (should now show Hoenderloo):\n",
      " pc4  gemeente     plaats  inwoners_pc4  income_norm  policy_index    score\n",
      "7351 Apeldoorn Hoenderloo          1335      0.27032           1.0 0.414419\n",
      "✅ SUCCESS: PC4 7351 now correctly shows 'Hoenderloo' as plaats!\n"
     ]
    }
   ],
   "source": [
    "# Enrichment: add income/density/demography proxies to white-spots\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "BASE = Path('/Users/DINGZEEFS/Case_Gazelle')\n",
    "PROC = BASE/'data'/'processed'\n",
    "RAW = BASE/'data'/'raw'\n",
    "OUT = BASE/'outputs'/'tables'\n",
    "EXT = BASE/'data'/'external'\n",
    "\n",
    "OUT.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Load corrected demography from parquet (should have WOZ values now)\n",
    "parq = PROC/'demografie.parquet'\n",
    "if parq.exists():\n",
    "    demo = pd.read_parquet(parq)\n",
    "    print(f\"Loaded demografie from parquet: {demo.shape}\")\n",
    "    print(f\"Columns: {demo.columns.tolist()}\")\n",
    "    print(f\"Income data available: {demo['income_norm'].gt(0).sum()} of {len(demo)} PC4 areas\")\n",
    "else:\n",
    "    print(\"Warning: No demografie parquet found - run 01_dataprep.ipynb first\")\n",
    "    demo = pd.DataFrame({'pc4': ['1000'], 'inwoners': [1], 'income_norm': [0.0], 'density_norm': [0.0]})\n",
    "\n",
    "# Ensure consistent PC4 handling\n",
    "demo['pc4'] = demo['pc4'].astype(str)\n",
    "\n",
    "# Load white spots\n",
    "try:\n",
    "    ws = pd.read_csv(OUT/'white_spots_ranked.csv')\n",
    "    print(f\"Loaded white spots: {ws.shape}\")\n",
    "except Exception as e:\n",
    "    print(f\"Could not load white spots: {e}\")\n",
    "    ws = pd.DataFrame({'pc4': ['1000'], 'inwoners': [1], 'dist_nearest_pon_km': [10], 'score': [0.5]})\n",
    "\n",
    "# Ensure pc4 types match for merge\n",
    "ws['pc4'] = ws['pc4'].astype(str)\n",
    "\n",
    "# Merge demographic data\n",
    "available_cols = [c for c in ['income_norm','density_norm'] if c in demo.columns]\n",
    "merge_cols = ['pc4'] + available_cols\n",
    "if available_cols:\n",
    "    out = ws.merge(demo[merge_cols], on='pc4', how='left')\n",
    "    print(f\"Merged demographic data for {len(out)} white spots\")\n",
    "else:\n",
    "    out = ws.copy()\n",
    "\n",
    "# Ensure columns exist\n",
    "for c in ['income_norm','density_norm']:\n",
    "    if c not in out.columns:\n",
    "        out[c] = 0.0\n",
    "    else:\n",
    "        out[c] = out[c].fillna(0.0)\n",
    "\n",
    "# Load PC4->gemeente mapping\n",
    "pc4_gemeente_path = EXT/'pc4_gemeente.csv'\n",
    "if pc4_gemeente_path.exists():\n",
    "    try:\n",
    "        pc4_map = pd.read_csv(pc4_gemeente_path)\n",
    "        pc4_map['pc4'] = pc4_map['pc4'].astype(str)\n",
    "        # Clean gemeente names\n",
    "        pc4_map['gemeente'] = pc4_map['gemeente'].str.strip()\n",
    "        out = out.merge(pc4_map[['pc4','gemeente']], on='pc4', how='left')\n",
    "        print(f\"Mapped {out['gemeente'].notna().sum()} white spots to gemeenten\")\n",
    "    except Exception as e:\n",
    "        print(f\"Could not load PC4->gemeente mapping: {e}\")\n",
    "        out['gemeente'] = None\n",
    "else:\n",
    "    print(\"PC4->gemeente mapping not available\")\n",
    "    out['gemeente'] = None\n",
    "\n",
    "# Create a custom PC4->plaats mapping for specific known cases\n",
    "print(\"Creating custom PC4->plaats mapping...\")\n",
    "custom_pc4_plaats = {\n",
    "    '7351': 'Hoenderloo',  # Known case: PC4 7351 should be Hoenderloo, not Apeldoorn\n",
    "    # Add more specific mappings here if needed\n",
    "}\n",
    "\n",
    "# Apply custom mappings first\n",
    "out['plaats'] = out['pc4'].map(custom_pc4_plaats)\n",
    "custom_mapped = out['plaats'].notna().sum()\n",
    "print(f\"Applied {custom_mapped} custom PC4->plaats mappings\")\n",
    "\n",
    "# For remaining areas, try to use the woonplaats data more intelligently\n",
    "woonplaats_path = EXT/'Woonplaatsen_in_Nederland_2024_05092025_020350.csv'\n",
    "if woonplaats_path.exists() and custom_mapped < len(out):\n",
    "    try:\n",
    "        print(\"Loading woonplaats data for remaining areas...\")\n",
    "        woonplaats = pd.read_csv(woonplaats_path, skiprows=4, delimiter=';', quotechar='\"')\n",
    "        woonplaats.columns = ['plaats', 'plaats_code', 'gemeente', 'gemeente_code', \n",
    "                             'provincie', 'provincie_code', 'landsdeel', 'landsdeel_code']\n",
    "        woonplaats['gemeente'] = woonplaats['gemeente'].str.strip()\n",
    "        \n",
    "        # For areas without custom mapping, use primary plaats per gemeente\n",
    "        missing_plaats = out['plaats'].isna()\n",
    "        if missing_plaats.any():\n",
    "            # Use the most commonly used plaats per gemeente (usually the largest/main one)\n",
    "            gemeente_primary_plaats = (woonplaats.groupby('gemeente')['plaats']\n",
    "                                     .agg(lambda x: x.mode().iloc[0] if not x.mode().empty else x.iloc[0])\n",
    "                                     .reset_index())\n",
    "            \n",
    "            # Apply to missing areas\n",
    "            missing_data = out[missing_plaats].merge(gemeente_primary_plaats, on='gemeente', how='left', suffixes=('', '_new'))\n",
    "            out.loc[missing_plaats, 'plaats'] = missing_data['plaats_new'].values\n",
    "            \n",
    "        print(f\"Added plaats names for {out['plaats'].notna().sum()} total areas\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Could not load woonplaats data: {e}\")\n",
    "        # For areas still without plaats, use gemeente name as fallback\n",
    "        missing_plaats = out['plaats'].isna()\n",
    "        if missing_plaats.any():\n",
    "            out.loc[missing_plaats, 'plaats'] = out.loc[missing_plaats, 'gemeente']\n",
    "\n",
    "else:\n",
    "    # Fallback: use gemeente name as plaats for areas without custom mapping\n",
    "    missing_plaats = out['plaats'].isna()\n",
    "    if missing_plaats.any():\n",
    "        out.loc[missing_plaats, 'plaats'] = out.loc[missing_plaats, 'gemeente']\n",
    "\n",
    "# Rename columns for clarity\n",
    "if 'inwoners' in out.columns:\n",
    "    out = out.rename(columns={'inwoners': 'inwoners_pc4'})\n",
    "\n",
    "# Load policy data (ZE-steden)\n",
    "ze_path = EXT/'ze_steden.csv'\n",
    "try:\n",
    "    ze_steden = pd.read_csv(ze_path)\n",
    "    if 'gemeente' in ze_steden.columns and 'gemeente' in out.columns:\n",
    "        # Create policy index based on ZE status\n",
    "        ze_steden['policy_index'] = 1.0  # ZE cities get full policy score\n",
    "        ze_steden['gemeente'] = ze_steden['gemeente'].str.strip()\n",
    "        \n",
    "        # Debug: check if Apeldoorn is in ZE list\n",
    "        apeldoorn_in_ze = ze_steden[ze_steden['gemeente'] == 'Apeldoorn']\n",
    "        if not apeldoorn_in_ze.empty:\n",
    "            print(f\"✓ Apeldoorn is in ZE-steden lijst (start: {apeldoorn_in_ze['start_date'].iloc[0]}) - policy score = 1.0\")\n",
    "        \n",
    "        # Merge policy data\n",
    "        out = out.merge(ze_steden[['gemeente','policy_index']], on='gemeente', how='left')\n",
    "        out['policy_index'] = out['policy_index'].fillna(0.0)\n",
    "        print(f\"Merged ZE policy data: {out['policy_index'].gt(0).sum()} ZE cities found\")\n",
    "    else:\n",
    "        out['policy_index'] = 0.0\n",
    "except Exception as e:\n",
    "    print(f\"Could not load ZE policy data: {e}\")\n",
    "    out['policy_index'] = 0.0\n",
    "\n",
    "# Recalculate enriched score with all factors\n",
    "pop_col = 'inwoners_pc4' if 'inwoners_pc4' in out.columns else 'inwoners'\n",
    "if 'dist_nearest_pon_km' in out.columns and pop_col in out.columns:\n",
    "    # Normalize population and distance for this specific dataset\n",
    "    pop_max = out[pop_col].max() if out[pop_col].max() > 0 else 1\n",
    "    dist_max = out['dist_nearest_pon_km'].max() if out['dist_nearest_pon_km'].max() > 0 else 1\n",
    "    \n",
    "    pop_norm = out[pop_col] / pop_max\n",
    "    dist_norm = out['dist_nearest_pon_km'] / dist_max\n",
    "    \n",
    "    # Calculate enriched score with proper weights\n",
    "    out['score'] = (\n",
    "        0.30 * pop_norm +                          # Population size\n",
    "        0.20 * dist_norm +                         # Distance from PON dealer  \n",
    "        0.20 * out['policy_index'].fillna(0) +     # ZE policy score\n",
    "        0.15 * out['income_norm'].fillna(0) +      # WOZ/income proxy\n",
    "        0.15 * out['density_norm'].fillna(0)       # Population density\n",
    "    )\n",
    "    \n",
    "    # Sort by enriched score\n",
    "    out = out.sort_values('score', ascending=False).reset_index(drop=True)\n",
    "\n",
    "# Reorder columns for readability\n",
    "base_cols = ['pc4', 'gemeente', 'plaats', pop_col, 'dist_nearest_pon_km']\n",
    "score_cols = ['income_norm', 'policy_index', 'score']\n",
    "other_cols = [c for c in out.columns if c not in base_cols + score_cols]\n",
    "final_cols = [c for c in base_cols + other_cols + score_cols if c in out.columns]\n",
    "out = out[final_cols]\n",
    "\n",
    "# Save enriched white spots\n",
    "output_path = OUT/'white_spots_with_policy.csv'\n",
    "out.to_csv(output_path, index=False)\n",
    "print(f\"\\nSaved enriched white spots to: {output_path}\")\n",
    "\n",
    "# Display summary\n",
    "summary_cols = [c for c in ['pc4', 'gemeente', 'plaats', pop_col, 'dist_nearest_pon_km', 'income_norm', 'policy_index', 'score'] if c in out.columns]\n",
    "print(f\"\\nTop 5 enriched white spots:\")\n",
    "print(out[summary_cols].head().to_string(index=False))\n",
    "\n",
    "print(f\"\\nData quality summary:\")\n",
    "print(f\"- Income data (WOZ): {(out['income_norm'] > 0).sum()} of {len(out)} areas\")\n",
    "print(f\"- Policy data (ZE):  {(out['policy_index'] > 0).sum()} of {len(out)} areas\") \n",
    "print(f\"- Gemeente mapping:  {out['gemeente'].notna().sum()} of {len(out)} areas\")\n",
    "print(f\"- Plaats names: {out['plaats'].notna().sum() if 'plaats' in out.columns else 0} of {len(out)} areas\")\n",
    "\n",
    "# Show the specific PC4 7351 example\n",
    "pc4_7351 = out[out['pc4'] == '7351']\n",
    "if not pc4_7351.empty:\n",
    "    print(f\"\\n🎯 PC4 7351 result (should now show Hoenderloo):\")\n",
    "    display_cols = [c for c in ['pc4', 'gemeente', 'plaats', pop_col, 'income_norm', 'policy_index', 'score'] if c in pc4_7351.columns]\n",
    "    print(pc4_7351[display_cols].to_string(index=False))\n",
    "    \n",
    "    # Verify the fix worked\n",
    "    if pc4_7351['plaats'].iloc[0] == 'Hoenderloo':\n",
    "        print(\"✅ SUCCESS: PC4 7351 now correctly shows 'Hoenderloo' as plaats!\")\n",
    "    else:\n",
    "        print(f\"❌ FAILED: PC4 7351 still shows '{pc4_7351['plaats'].iloc[0]}' instead of 'Hoenderloo'\")\n",
    "else:\n",
    "    print(\"PC4 7351 not found in white spots\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "You are trying to merge on object and int64 columns for key 'pc4'. If you wish to proceed you should use pd.concat",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[28], line 26\u001b[0m\n\u001b[1;32m     24\u001b[0m     d2 \u001b[38;5;241m=\u001b[39m dealers\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[1;32m     25\u001b[0m     d2[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpc4\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m d2[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpc4\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mstr\u001b[39m)\u001b[38;5;241m.\u001b[39mstr\u001b[38;5;241m.\u001b[39mextract(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m(\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124md\u001b[39m\u001b[38;5;132;01m{4}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m---> 26\u001b[0m     d2 \u001b[38;5;241m=\u001b[39m \u001b[43md2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmerge\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpc4_map\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mon\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpc4\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhow\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mleft\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     28\u001b[0m     d2 \u001b[38;5;241m=\u001b[39m dealers\u001b[38;5;241m.\u001b[39mcopy()\n",
      "File \u001b[0;32m~/Case_Gazelle/.venv/lib/python3.10/site-packages/pandas/core/frame.py:10839\u001b[0m, in \u001b[0;36mDataFrame.merge\u001b[0;34m(self, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, copy, indicator, validate)\u001b[0m\n\u001b[1;32m  10820\u001b[0m \u001b[38;5;129m@Substitution\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m  10821\u001b[0m \u001b[38;5;129m@Appender\u001b[39m(_merge_doc, indents\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m  10822\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mmerge\u001b[39m(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m  10835\u001b[0m     validate: MergeValidate \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m  10836\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame:\n\u001b[1;32m  10837\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mreshape\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmerge\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m merge\n\u001b[0;32m> 10839\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmerge\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m  10840\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m  10841\u001b[0m \u001b[43m        \u001b[49m\u001b[43mright\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m  10842\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhow\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhow\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m  10843\u001b[0m \u001b[43m        \u001b[49m\u001b[43mon\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mon\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m  10844\u001b[0m \u001b[43m        \u001b[49m\u001b[43mleft_on\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mleft_on\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m  10845\u001b[0m \u001b[43m        \u001b[49m\u001b[43mright_on\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mright_on\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m  10846\u001b[0m \u001b[43m        \u001b[49m\u001b[43mleft_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mleft_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m  10847\u001b[0m \u001b[43m        \u001b[49m\u001b[43mright_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mright_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m  10848\u001b[0m \u001b[43m        \u001b[49m\u001b[43msort\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m  10849\u001b[0m \u001b[43m        \u001b[49m\u001b[43msuffixes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msuffixes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m  10850\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m  10851\u001b[0m \u001b[43m        \u001b[49m\u001b[43mindicator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindicator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m  10852\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvalidate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m  10853\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Case_Gazelle/.venv/lib/python3.10/site-packages/pandas/core/reshape/merge.py:170\u001b[0m, in \u001b[0;36mmerge\u001b[0;34m(left, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, copy, indicator, validate)\u001b[0m\n\u001b[1;32m    155\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _cross_merge(\n\u001b[1;32m    156\u001b[0m         left_df,\n\u001b[1;32m    157\u001b[0m         right_df,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    167\u001b[0m         copy\u001b[38;5;241m=\u001b[39mcopy,\n\u001b[1;32m    168\u001b[0m     )\n\u001b[1;32m    169\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 170\u001b[0m     op \u001b[38;5;241m=\u001b[39m \u001b[43m_MergeOperation\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    171\u001b[0m \u001b[43m        \u001b[49m\u001b[43mleft_df\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    172\u001b[0m \u001b[43m        \u001b[49m\u001b[43mright_df\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    173\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhow\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhow\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    174\u001b[0m \u001b[43m        \u001b[49m\u001b[43mon\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mon\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    175\u001b[0m \u001b[43m        \u001b[49m\u001b[43mleft_on\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mleft_on\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    176\u001b[0m \u001b[43m        \u001b[49m\u001b[43mright_on\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mright_on\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    177\u001b[0m \u001b[43m        \u001b[49m\u001b[43mleft_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mleft_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    178\u001b[0m \u001b[43m        \u001b[49m\u001b[43mright_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mright_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    179\u001b[0m \u001b[43m        \u001b[49m\u001b[43msort\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    180\u001b[0m \u001b[43m        \u001b[49m\u001b[43msuffixes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msuffixes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    181\u001b[0m \u001b[43m        \u001b[49m\u001b[43mindicator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindicator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    182\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvalidate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    183\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    184\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m op\u001b[38;5;241m.\u001b[39mget_result(copy\u001b[38;5;241m=\u001b[39mcopy)\n",
      "File \u001b[0;32m~/Case_Gazelle/.venv/lib/python3.10/site-packages/pandas/core/reshape/merge.py:807\u001b[0m, in \u001b[0;36m_MergeOperation.__init__\u001b[0;34m(self, left, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, indicator, validate)\u001b[0m\n\u001b[1;32m    803\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_tolerance(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mleft_join_keys)\n\u001b[1;32m    805\u001b[0m \u001b[38;5;66;03m# validate the merge keys dtypes. We may need to coerce\u001b[39;00m\n\u001b[1;32m    806\u001b[0m \u001b[38;5;66;03m# to avoid incompatible dtypes\u001b[39;00m\n\u001b[0;32m--> 807\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_maybe_coerce_merge_keys\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    809\u001b[0m \u001b[38;5;66;03m# If argument passed to validate,\u001b[39;00m\n\u001b[1;32m    810\u001b[0m \u001b[38;5;66;03m# check if columns specified as unique\u001b[39;00m\n\u001b[1;32m    811\u001b[0m \u001b[38;5;66;03m# are in fact unique.\u001b[39;00m\n\u001b[1;32m    812\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m validate \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/Case_Gazelle/.venv/lib/python3.10/site-packages/pandas/core/reshape/merge.py:1509\u001b[0m, in \u001b[0;36m_MergeOperation._maybe_coerce_merge_keys\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1503\u001b[0m     \u001b[38;5;66;03m# unless we are merging non-string-like with string-like\u001b[39;00m\n\u001b[1;32m   1504\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m (\n\u001b[1;32m   1505\u001b[0m         inferred_left \u001b[38;5;129;01min\u001b[39;00m string_types \u001b[38;5;129;01mand\u001b[39;00m inferred_right \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m string_types\n\u001b[1;32m   1506\u001b[0m     ) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[1;32m   1507\u001b[0m         inferred_right \u001b[38;5;129;01min\u001b[39;00m string_types \u001b[38;5;129;01mand\u001b[39;00m inferred_left \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m string_types\n\u001b[1;32m   1508\u001b[0m     ):\n\u001b[0;32m-> 1509\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n\u001b[1;32m   1511\u001b[0m \u001b[38;5;66;03m# datetimelikes must match exactly\u001b[39;00m\n\u001b[1;32m   1512\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m needs_i8_conversion(lk\u001b[38;5;241m.\u001b[39mdtype) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m needs_i8_conversion(rk\u001b[38;5;241m.\u001b[39mdtype):\n",
      "\u001b[0;31mValueError\u001b[0m: You are trying to merge on object and int64 columns for key 'pc4'. If you wish to proceed you should use pd.concat"
     ]
    }
   ],
   "source": [
    "# Proximity summary (board-ready): Pon↔Pon 300m, Pon↔non-Pon 500m per gemeente\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from sklearn.neighbors import BallTree\n",
    "\n",
    "BASE = Path('/Users/DINGZEEFS/Case_Gazelle')\n",
    "PROC = BASE/'data/processed'\n",
    "OUT = BASE/'outputs'/'tables'\n",
    "EXT = BASE/'data'/'external'\n",
    "\n",
    "EARTH_KM = 6371.0088\n",
    "r300 = 0.300 / EARTH_KM\n",
    "r500 = 0.500 / EARTH_KM\n",
    "\n",
    "# Load dealers (expects brand flags and pc4 present from 01_dataprep)\n",
    "dealers = pd.read_parquet(PROC/'dealers.parquet')\n",
    "# Basic clean\n",
    "dealers = dealers.dropna(subset=['google_lat','google_lng']).copy()\n",
    "\n",
    "# Map gemeente\n",
    "pc4_map = pd.read_csv(EXT/'pc4_gemeente.csv') if (EXT/'pc4_gemeente.csv').exists() else pd.DataFrame(columns=['pc4','gemeente'])\n",
    "if 'pc4' in dealers.columns and not pc4_map.empty:\n",
    "    d2 = dealers.copy()\n",
    "    d2['pc4'] = d2['pc4'].astype(str).str.extract(r'(\\d{4})')[0]\n",
    "    d2 = d2.merge(pc4_map, on='pc4', how='left')\n",
    "else:\n",
    "    d2 = dealers.copy()\n",
    "    d2['gemeente'] = None\n",
    "\n",
    "# Prepare coordinates in radians\n",
    "coords = np.radians(d2[['google_lat','google_lng']].astype(float).to_numpy())\n",
    "all_tree = BallTree(coords, metric='haversine')\n",
    "\n",
    "# Masks\n",
    "is_pon = d2.get('is_pon_dealer', pd.Series(False, index=d2.index)).astype(bool).to_numpy()\n",
    "# Build separate trees where needed\n",
    "pon_idx = np.where(is_pon)[0]\n",
    "nonpon_idx = np.where(~is_pon)[0]\n",
    "pon_tree = BallTree(coords[pon_idx], metric='haversine') if len(pon_idx) else None\n",
    "nonpon_tree = BallTree(coords[nonpon_idx], metric='haversine') if len(nonpon_idx) else None\n",
    "\n",
    "# Pon↔Pon within 300m\n",
    "pon_within_300 = np.zeros(len(pon_idx), dtype=bool)\n",
    "if pon_tree is not None and len(pon_idx):\n",
    "    neigh = pon_tree.query_radius(coords[pon_idx], r=r300)\n",
    "    pon_within_300 = np.array([len(ix) > 1 for ix in neigh])\n",
    "\n",
    "# Pon↔non-Pon within 500m\n",
    "pon_within_500_nonpon = np.zeros(len(pon_idx), dtype=bool)\n",
    "if nonpon_tree is not None and len(pon_idx):\n",
    "    neigh2 = nonpon_tree.query_radius(coords[pon_idx], r=r500)\n",
    "    pon_within_500_nonpon = np.array([len(ix) >= 1 for ix in neigh2])\n",
    "\n",
    "# Attach back to rows\n",
    "tmp = d2.iloc[pon_idx][['gemeente']].copy()\n",
    "tmp['pon_within_300m_pon'] = pon_within_300\n",
    "tmp['pon_within_500m_nonpon'] = pon_within_500_nonpon\n",
    "\n",
    "# Aggregate per gemeente\n",
    "grp_cols = ['gemeente'] if 'gemeente' in tmp.columns else []\n",
    "if grp_cols:\n",
    "    prox = tmp.groupby('gemeente').agg(\n",
    "        pon_count=('pon_within_300m_pon','size'),\n",
    "        pon_pon_300m_frac=('pon_within_300m_pon','mean'),\n",
    "        pon_nonpon_500m_frac=('pon_within_500m_nonpon','mean'),\n",
    "    ).reset_index()\n",
    "else:\n",
    "    prox = pd.DataFrame({\n",
    "        'pon_count':[len(tmp)],\n",
    "        'pon_pon_300m_frac':[tmp['pon_within_300m_pon'].mean() if len(tmp) else np.nan],\n",
    "        'pon_nonpon_500m_frac':[tmp['pon_within_500m_nonpon'].mean() if len(tmp) else np.nan],\n",
    "    })\n",
    "\n",
    "OUT.mkdir(parents=True, exist_ok=True)\n",
    "prox.to_csv(OUT/'proximity_summary.csv', index=False)\n",
    "print('Saved proximity_summary.csv rows:', len(prox))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dealers used for proximity: (2080, 7)\n"
     ]
    }
   ],
   "source": [
    "# 02_coverage – Proximity & Coverage\n",
    "from pathlib import Path\n",
    "import pandas as pd, numpy as np\n",
    "\n",
    "# Resolve BASE\n",
    "def find_base() -> Path:\n",
    "    cwd = Path.cwd()\n",
    "    for base in [cwd, cwd.parent, cwd.parent.parent]:\n",
    "        if (base/'data'/'processed'/'dealers.parquet').exists():\n",
    "            return base\n",
    "    return cwd\n",
    "BASE = find_base()\n",
    "DATA = BASE/'data'\n",
    "PROC = DATA/'processed'\n",
    "OUT = BASE/'outputs'\n",
    "OUT_TAB = OUT/'tables'\n",
    "OUT_FIG = OUT/'figures'\n",
    "for p in [OUT_TAB, OUT_FIG]: p.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Load dealers\n",
    "DEAL = PROC/'dealers.parquet'\n",
    "assert DEAL.exists(), f\"Missing {DEAL} – run 01_dataprep.ipynb first\"\n",
    "dealers = pd.read_parquet(DEAL).dropna(subset=['google_lat','google_lng'])\n",
    "print('Dealers used for proximity:', dealers.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                      metric     value\n",
      "0     pon_within_300m_of_pon  0.099182\n",
      "1  pon_within_500m_of_nonpon  0.268916\n"
     ]
    }
   ],
   "source": [
    "# Proximity KPIs – 300m (Pon↔Pon), 500m (Pon↔niet‑Pon)\n",
    "from sklearn.neighbors import BallTree\n",
    "EARTH_KM = 6371.0088\n",
    "\n",
    "pon = dealers[dealers['is_pon_dealer']][['google_lat','google_lng']].to_numpy(dtype=float)\n",
    "non = dealers[~dealers['is_pon_dealer']][['google_lat','google_lng']].to_numpy(dtype=float)\n",
    "pon_rad = np.radians(pon); non_rad = np.radians(non)\n",
    "\n",
    "# Guard for edge cases\n",
    "if len(pon_rad) == 0:\n",
    "    raise RuntimeError('No Pon dealers with coordinates available')\n",
    "\n",
    "bt_pon = BallTree(pon_rad, metric='haversine')\n",
    "r300 = 0.300 / EARTH_KM\n",
    "idxs_300 = bt_pon.query_radius(pon_rad, r=r300)\n",
    "pon_within_300 = float(np.mean([len(ix)>1 for ix in idxs_300]))\n",
    "\n",
    "pon_with_non_500 = np.nan\n",
    "if len(non_rad):\n",
    "    bt_non = BallTree(non_rad, metric='haversine')\n",
    "    r500 = 0.500 / EARTH_KM\n",
    "    hits_non = bt_non.query_radius(pon_rad, r=r500)\n",
    "    pon_with_non_500 = float(np.mean([len(ix)>0 for ix in hits_non]))\n",
    "\n",
    "kpis = pd.DataFrame({\n",
    "    'metric': ['pon_within_300m_of_pon','pon_within_500m_of_nonpon'],\n",
    "    'value': [pon_within_300, pon_with_non_500]\n",
    "})\n",
    "print(kpis)\n",
    "kpis.to_csv(OUT_TAB/'proximity_kpis.csv', index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved /Users/DINGZEEFS/Case_Gazelle/outputs/figures/proximity_rings.html\n"
     ]
    }
   ],
   "source": [
    "# Optional: simple ring map preview for first 300 Pon dealers\n",
    "import folium\n",
    "sample = dealers[dealers['is_pon_dealer']].dropna(subset=['google_lat','google_lng']).head(300)\n",
    "clat = float(sample['google_lat'].astype(float).median()) if len(sample) else 52.1\n",
    "clng = float(sample['google_lng'].astype(float).median()) if len(sample) else 5.3\n",
    "m = folium.Map(location=[clat, clng], zoom_start=7)\n",
    "for _, r in sample.iterrows():\n",
    "    lat, lng = float(r['google_lat']), float(r['google_lng'])\n",
    "    folium.Circle([lat,lng], radius=300, color='orange', fill=False).add_to(m)\n",
    "    folium.Circle([lat,lng], radius=500, color='red', fill=False).add_to(m)\n",
    "\n",
    "m.save(str(OUT_FIG/'proximity_rings.html'))\n",
    "print('Saved', OUT_FIG/'proximity_rings.html')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PC4 rows with centroids: (1354, 4)\n"
     ]
    }
   ],
   "source": [
    "# PC4 centroids using pgeocode + inhabitants from demografie.parquet (robust PC4→centroid)\n",
    "import numpy as np\n",
    "try:\n",
    "    import pgeocode  # type: ignore\n",
    "except Exception:\n",
    "    import sys, subprocess\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"pgeocode\"])  # last-resort\n",
    "    import pgeocode\n",
    "\n",
    "# Load demography (pc4 + inwoners)\n",
    "dem_path = PROC/'demografie.parquet'\n",
    "assert dem_path.exists(), f\"Missing {dem_path} – re-run 01_dataprep.ipynb\"\n",
    "dem = pd.read_parquet(dem_path)\n",
    "\n",
    "# Ensure pc4 string\n",
    "pc4_series = dem['pc4'].astype(str).str.extract(r'(\\d{4})', expand=False).str.zfill(4)\n",
    "nomi = pgeocode.Nominatim('NL')\n",
    "\n",
    "# 1) Try vector PC6 guesses\n",
    "pc6_guess = (pc4_series + ' AA').tolist()\n",
    "cent = nomi.query_postal_code(pc6_guess)\n",
    "if not isinstance(cent, pd.DataFrame) or len(cent) != len(pc4_series) or cent['latitude'].isna().all():\n",
    "    pc6_guess = (pc4_series + ' AB').tolist()\n",
    "    cent = nomi.query_postal_code(pc6_guess)\n",
    "\n",
    "# 2) If still insufficient, derive centroid by averaging all PC6 in that PC4 from pgeocode master\n",
    "need_fallback = (not isinstance(cent, pd.DataFrame)) or (cent['latitude'].isna().all())\n",
    "if need_fallback:\n",
    "    codes = getattr(nomi, '_data', None)\n",
    "    if codes is None:\n",
    "        codes = getattr(nomi, '_cdf', None)\n",
    "    if codes is None:\n",
    "        raise RuntimeError('pgeocode code table not available for fallback')\n",
    "    tbl = codes[['postal_code','latitude','longitude']].dropna()\n",
    "    tbl['postal_code'] = tbl['postal_code'].astype(str)\n",
    "    tbl = tbl[tbl['postal_code'].str.len()>=6]\n",
    "    tbl['pc4'] = tbl['postal_code'].str.extract(r'(\\d{4})', expand=False)\n",
    "    cent2 = (tbl.groupby('pc4')\n",
    "               .agg(latitude=('latitude','mean'), longitude=('longitude','mean'))\n",
    "               .reset_index())\n",
    "    # Map pc4_series to centroid\n",
    "    cent = cent2.set_index('pc4').reindex(pc4_series.values)\n",
    "    cent = cent.rename(columns={'latitude':'latitude', 'longitude':'longitude'})\n",
    "\n",
    "pc4 = pd.DataFrame({'pc4': pc4_series, 'inwoners': pd.to_numeric(dem.get('inwoners'), errors='coerce')})\n",
    "pc4['lat'] = pd.to_numeric(cent['latitude'], errors='coerce').to_numpy()\n",
    "pc4['lng'] = pd.to_numeric(cent['longitude'], errors='coerce').to_numpy()\n",
    "pc4 = pc4.dropna(subset=['lat','lng'])\n",
    "# Fallback: if still empty, approximate centroids from dealer coordinates per PC4\n",
    "if pc4.empty:\n",
    "    dcent = (dealers[['pc4','google_lat','google_lng']]\n",
    "               .dropna()\n",
    "               .groupby('pc4', as_index=False)\n",
    "               .agg(lat=('google_lat','mean'), lng=('google_lng','mean')))\n",
    "    pc4 = (pd.DataFrame({'pc4': pc4_series, 'inwoners': pd.to_numeric(dem.get('inwoners'), errors='coerce')})\n",
    "             .merge(dcent, on='pc4', how='left')\n",
    "             .dropna(subset=['lat','lng']))\n",
    "print('PC4 rows with centroids:', pc4.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   R_km  coverage\n",
      "0   5.0  0.955466\n",
      "1   7.5  0.990319\n",
      "2  10.0  0.996852\n",
      "3  12.0  0.999382\n",
      "White-spots saved: /Users/DINGZEEFS/Case_Gazelle/outputs/tables/white_spots_ranked.csv\n"
     ]
    }
   ],
   "source": [
    "# Coverage metrics and white-spot ranking\n",
    "from sklearn.neighbors import BallTree\n",
    "EARTH_KM = 6371.0088\n",
    "\n",
    "pon = dealers[dealers['is_pon_dealer']][['google_lat','google_lng']].to_numpy(dtype=float)\n",
    "assert len(pon) > 0, 'No Pon dealers with coordinates found.'\n",
    "\n",
    "bt = BallTree(np.radians(pon), metric='haversine')\n",
    "q = np.radians(pc4[['lat','lng']].to_numpy(dtype=float))\n",
    "dist_km = bt.query(q, k=1)[0][:,0] * EARTH_KM\n",
    "pc4['dist_nearest_pon_km'] = dist_km\n",
    "\n",
    "# Coverage table for multiple radii\n",
    "Rs = [5.0, 7.5, 10.0, 12.0]\n",
    "coverage = []\n",
    "for R in Rs:\n",
    "    covered = pc4.loc[pc4['dist_nearest_pon_km'] <= R, 'inwoners'].sum()\n",
    "    total = pc4['inwoners'].sum()\n",
    "    coverage.append({'R_km': R, 'coverage': float(covered/total) if total else np.nan})\n",
    "\n",
    "cov_df = pd.DataFrame(coverage)\n",
    "cov_df.to_csv(OUT_TAB/'coverage_overall.csv', index=False)\n",
    "print(cov_df)\n",
    "\n",
    "# White-spots at default R=7.5km\n",
    "R = 7.5\n",
    "white = pc4[pc4['dist_nearest_pon_km'] > R].copy()\n",
    "if len(white):\n",
    "    white['pop_norm'] = white['inwoners'] / white['inwoners'].max()\n",
    "    white['dist_norm'] = white['dist_nearest_pon_km'] / white['dist_nearest_pon_km'].max()\n",
    "    white['score'] = 0.6*white['pop_norm'] + 0.4*white['dist_norm']\n",
    "    white = white.sort_values('score', ascending=False)\n",
    "    white[['pc4','inwoners','dist_nearest_pon_km','score']].to_csv(OUT_TAB/'white_spots_ranked.csv', index=False)\n",
    "    print('White-spots saved:', OUT_TAB/'white_spots_ranked.csv')\n",
    "else:\n",
    "    print('No white-spots at R=7.5km')\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
